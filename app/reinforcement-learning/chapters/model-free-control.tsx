import Link from "next/link";
import Image from "next/image";
import "katex/dist/katex.min.css";
import { InlineMath, BlockMath } from "react-katex";

export default function ModelFreeControl() {
  return (
    <section id="ch-five-model-free-control" className="mb-12">
      <h1 className="text-3xl font-bold mb-6 text-center">
        Model Free Control
      </h1>

      <h2 id="mfc-intro" className="text-2xl font-bold mb-6 text-center">
        Introduction
      </h2>

      <div className="mb-4 mt-24">
        <p className="mb-4">
          In the last section, we studied <strong>model-free prediction</strong>
          :
        </p>

        <p className="mb-6">
          Given a fixed policy <InlineMath math="\pi" />, how do we estimate its
          value function <InlineMath math="v_\pi" /> when we don&apos;t know the
          MDP structure (the transition probabilities or reward function)?
        </p>

        <p className="mb-4">
          We learned two main approaches: Monte Carlo methods, which average
          complete returns, and Temporal Difference learning, which bootstraps
          from current value estimates.
        </p>

        <p className="mb-4">But evaluation alone isn&apos;t enough.</p>

        <p className="mb-6">
          The real goal of reinforcement learning is not just to evaluate a
          policy, but to <em>find the best policy</em>.
        </p>

        <p className="mb-4">
          This section is about <strong>model-free control</strong>:
        </p>

        <p className="mb-6">
          How do we optimize the policy to find the best value function when we
          don&apos;t know the MDP structure?
        </p>

        <p className="mb-4">
          We&apos;ll explore three main approaches, organized as follows:
        </p>

        <ul className="list-disc list-inside mb-6 space-y-1 ml-4">
          <li>
            <strong>On-Policy Monte Carlo Control</strong>: Using Monte Carlo
            methods to improve policies
          </li>
          <li>
            <strong>On-Policy Temporal Difference Learning</strong>: Using TD
            methods (specifically SARSA) for online control
          </li>
          <li>
            <strong>Off-Policy Learning</strong>: Learning about one policy
            while following another (Q-learning)
          </li>
          <li>
            <strong>Summary</strong>: Bringing it all together
          </li>
        </ul>

        <p className="mb-4">
          <strong>On-Policy vs Off-Policy Learning</strong>
        </p>

        <p className="mb-4">
          Before we dive in, we need to understand a crucial distinction in
          reinforcement learning:
        </p>

        <p className="mb-4">
          <strong>On-policy learning</strong> means &quot;learn on the job&quot;
        </p>

        <p className="mb-6">
          The agent learns about policy <InlineMath math="\pi" /> from
          experience generated by following that same policy{" "}
          <InlineMath math="\pi" />.
        </p>

        <p className="mb-4">
          The policy being evaluated and the policy being used to generate
          behavior are the same.
        </p>

        <p className="mb-4">
          <strong>Off-policy learning</strong> means &quot;look over
          someone&apos;s shoulder&quot;
        </p>

        <p className="mb-6">
          The agent learns about policy <InlineMath math="\pi" /> (the target
          policy) from experience generated by following a different policy{" "}
          <InlineMath math="\mu" /> (the behavior policy).
        </p>

        <p className="mb-4">
          This is like learning to be a good driver by watching someone else
          drive, even if they&apos;re not driving optimally.
        </p>

        <p className="mb-4">
          Off-policy methods are more general and flexible, but they&apos;re
          also more complex.
        </p>

        <p className="mb-4">
          We&apos;ll start with on-policy methods because they&apos;re simpler
          to understand, then move to off-policy methods once we have the
          foundations in place.
        </p>
      </div>
    </section>
  );
}
