import Link from "next/link";
import Image from "next/image";
import "katex/dist/katex.min.css";
import { InlineMath, BlockMath } from "react-katex";

export default function MarkovDecisionProcess() {
  return (
    <section id="ch-two-mdp" className="mb-12">
      <h1 className="text-3xl font-bold mb-6 text-center">
        Markov Decision Process
      </h1>
      <h2 id="mdp-intro" className="text-2xl font-bold mb-6 text-center">
        Introduction
      </h2>
      <p className="mb-4">
        At a very high level, Reinforcement Learning is really about dealing
        with sequences of dependent events and wanting to learn useful
        information about these events by using probability. There are two broad
        kinds of sequences of events: the first is sequences of{" "}
        <strong> independent events</strong>, things like coin flips or dice
        rolls, where each outcome has nothing to do with the one before it.
        Classical probability theory (pre 1900) was built for this world, and it
        works beautifully there. The second kind are sequences of{" "}
        <strong>dependent events</strong>, where what happens next depends on
        what just happened, and what happened before that, etc. This is the
        world most real problems live in. The challenge is that dependence makes
        classical probability messy and hard to work with. MDPs exist to solve
        exactly this problem. They give us a mathematical framework for using
        probability in a world where events are not independent. One key idea is
        to compress all the relevant information from the past into a single
        object called the <strong>state</strong>. Once you do that, you can
        treat the future as depending only on the present state (instead of the
        entire history). So, if you have a problem that deals with sequences of
        dependent events, and you want to apply probability theory to them (like
        in RL), MDPs are the mathematics you need.
      </p>

      <p className="mb-4">
        <strong id="why-history-matters" className="block text-center">
          A Quick Detour: Why Learn The History Of Math
        </strong>
        <br />
        Before I jump into the math, it&apos;s worth asking a slightly
        philosophical question: why bother learning the history behind these
        ideas at all? Why not just memorize the formulas and move on? The answer
        is that our brains are terrible at remembering abstract symbols in
        isolation, but they are very good at remembering stories. When you know{" "}
        <em>why</em> an idea was invented, what problem it was trying to solve,
        and what kind of human was behind it, it becomes much easier to both
        learn and remember. The other reason is that I enjoy it and I&apos;m the
        one writing this!
      </p>

      <p className="mb-4">
        <figure className="text-center my-14">
          <Image
            src="/images/reinforcement-learning/mdp/mdp-Andrey-Markov.png"
            alt="Picture of Andrey Markov"
            width={500}
            height={300}
            className="mx-auto"
          />
          <figcaption className="text-lg mt-2">Andrey Markov</figcaption>
        </figure>
        <strong id="mdp-history" className="block text-center">
          A History of MDPs
        </strong>
        <br />
        The origin of the MDP began at the dawn of the the 20th century in
        Russia and centers around a feud between Andrey Markov and Pavel
        Nekrasov. The powerful European Russian Empire had just been humiliated
        by Japan after losing the 1905 Russo-Japanese war, and socialist groups
        all across Russia began a revolution (not that revolution) against the
        tsar. This fiercely divided the empire into two political camps, the pro
        Tsarists (usually religious and conservative) and the anti Tsarists
        (usually socialist and atheist). In the world of mathematics, the most
        prominent pro tsar mathematician was the extremely religious Pavel
        Nekrasov. He wanted to use probability theory to prove the existence of
        free will and the will of God. Andrey Markov was a Russian
        mathematician, who was both an atheist and deeply anti Tsarit, and for
        this reason set out to discover an entirely new branch of mathematics in
        order to prove Nekrasov wrong.
      </p>

      <p className="mb-4">
        <br />A good summary can be found here:
        <br />
        <Link
          href="https://www.youtube.com/watch?v=KZeIEiBrT_w&t=1210s"
          target="_blank"
          className="text-blue-600 underline hover:text-blue-400"
        >
          The Strange Math That Predicts (Almost) Anything
        </Link>
        <br />
        <br />
        The feud centered around the Law of Large Numbers (the average outcome
        of a large amount of independent trials converges to the expected
        value), first proven by Jacob Bernouli in 1713. As an example, we know
        that the probability of a coin flip landing on heads is 50% and landing
        on tails is 50%, so even though after a few coin flips (say 10) we might
        have 7 heads and only 3 tails, if you keep increasing the amount of coin
        flips that you do, it will eventually converge to 50% heads and 50%
        tails. The Law of Large Numbers was the most important concept of
        probability theory until Andrey Markov&apos;s Markov Process. Bernouli
        had proved that the Law of Large Numbers applied to INDEPENDENT events,
        but it was an open question as to whether it also applied to DEPENDENT
        events. Nekrasov argued that it didn&apos;t, and that if you observed
        the Law of Large Numbers, you could infer that the underlying events
        MUST be independent. Nekrasov looked at large amounts of data about
        marriage rates, crime rates, and birth rates and saw that they seemed to
        follow the law of large numbers. This data came from decisions to get
        married, decisions to commit crime, and decisions to have children. Thus
        because they followed the law of large numbers, the decisions causing
        them MUST be independent (since he believed only independent events
        could follow the Law of Large Numbers). Because they were independent,
        they must be decisions of free will, and hence free will existed and was
        something that could be measured mathamatically using the Law of Large
        Numbers.
      </p>

      <p className="mb-4">
        Markov then set out to prove that Nekrasov&apos;s claims were wrong and
        needed to show that dependent events could also follow the law of large
        numbers. This would break Nekrasov&apos;s link that because marriage,
        crime, and birth rates followed the law of large numbers, that they
        needed to be independent. This would prove to be a huge milestone in the
        world of probability because it would show that you can still use
        probability theory on dependent events (including RL).
      </p>

      <p className="mb-4">
        The first thing Markov needed was a situation where one event was
        clearly dependent on a previous event. He settled on using a very famous
        Russian poem, "Eugene Onegin", by the early 19th century anti Tsarist
        Russian poet (and father of modern Russian literature), Alexander
        Pushkin. The idea was that the probability of the next letter in the
        poem being a consonant or a vowel was clearly dependent upon what the
        current letter in the poem was. Once he showed that, all he would need
        to prove is that consonants and vowels in the poem followed the law of
        large numbers, and that would show that dependent events can also follow
        the law of large numbers (not just independent events).
      </p>

      <p className="mb-4">
        He took the first 20,000 letters in the poem, removed all the spaces and
        punctuation, and stitched them together into one enormous string of
        letters. He found that 57% of the letters were consonants and 43% were
        vowels. He then decided to have each letter paired with the one after
        it. Thus 4 possible combinations emerged:
        <br />
        <br />
        1) Consonant Consonant
        <br />
        2) Consonant Vowel
        <br />
        3) Vowel Consonant
        <br />
        4) Vowel Vowel.
        <br />
        <br />
        1st he had to demonstrate that each letter was dependent on the previous
        letter instead of independent. To do this he multiplied the
        probabilities by their states in order to show what the distribution
        would be if they were independent.
        <br />
        <br />
        1) For consonant consonant it was: (.57) * (.57 ) = .32 chance
        <br />
        2) For consonant vowel it was: (.57) * (.43) = .25 chance
        <br />
        3) For vowel consonant it was: (.43) * (.57) = .25 chance
        <br />
        4) For vowel vowel it was: (.43) * (.43) = .18 chance
        <br />
        The numbers he actually got were .19, .375, .375, and .06
        <br />
        <br />
        Since the numbers he got were extremely far away from what one would get
        for independent events with that large of a sample size, it can be
        inferred with extremely high probability that these were dependent, not
        independent, events (as common sense would imply).
      </p>
      <p className="mb-4">
        Now Markov needed to show that the law of large numbers was still valid
        even though each letter was dependent on the previous letter in the
        sequence. To do this he created what is now known as a{" "}
        <strong>Markov Chain</strong>. He had each letter be a state and found
        the transition probabilities between states. Transition probabilities
        describe the likelihood of moving from one state to another based only
        on the current state. For example, the probability that a vowel is
        followed by another vowel is one transition probability, while the
        probability that a vowel is followed by a consonant is another. This was
        remarkable because it meant that you could work with sequences of
        dependent events and still get meaningful, predictable results without
        needing to track the entire sequence. By calculating these
        probabilities, Markov could prove that even with dependencies between
        letters, the long-run frequencies still converge to predictable values
        (in this case, the overall proportions of vowels (43%) and consonants
        (57%) in the poem). Now he needed to find all the state transition
        probabilities. How did he do this? Let&apos;s take a look at the
        transition probability for vowel to vowel as an example.
        <br />
        <br />
        How did Markov calculate the transition probability from vowel to vowel?
        He looked at the actual text. The probability of any letter being a
        vowel was 0.43. The probability of two consecutive vowels was 0.06. He
        divided 0.06 by 0.43 to get 0.13. This tells us: given that the current
        letter is a vowel, there&apos;s a 13% chance the next letter is also a
        vowel (and therefore an 87% chance it&apos;s a consonant). He did this
        for all possible stat etransitions: consonant to consonant was 0.33,
        meaning consonant to vowel was 0.67. These state transition
        probabilities let him model how states link together in a sequence, a
        chain of states, hence the name Markov chain.
      </p>

      <p className="mb-4">
        When you follow these state transition probabilities and start at a
        random starting point in the poem, you initially get wildly different
        ratios for the number of consonants and vowels you have (just like with
        independent events like flipping a coin, you might initially get 6 heads
        and 2 tails instead of the 50/50 split predicted). However, given a long
        enough length of letters, you eventually converge to .43 for Vowels and
        .57 for Consonants (just like how with a large enough amount of coin
        flips you converge to a 50/50 split). Thus Markov had shown that the law
        of large numbers applied to this dependent system, showing that Nekrasov
        was wrong and that you cannot just assume if a system follows the law of
        large numbers that it must be independent. This was groundbreaking
        because it showed that dependent systems aren't just chaotic and
        unpredictable, but that they can actually be modeled with mathematical
        structure and have predictable long-term behavior. Before Markov,
        mathematicians had no stable way to handle dependent sequences of
        events, they would need the ENTIRE history in order to make
        calculations.
        <br />
        <br />
        To recap, there is a mathematical object, the Markov Chain, which has
        states (vowels and consonants) and state transition probabilites give
        you the probability of going from state one to state two (vowel to
        vowel, vowel to consonant, consonant to vowel, and consonant to
        consonant). Using this mathematical object, you can predict that the
        ratio of vowels and consonants to the total amount of letters will
        converge to a stable long term ratio (an analogue of the law of large
        numbers for dependent processes) of .43 for vowels and .57 for
        consonants. Hence, Markov used this Markov Chain to prove that dependent
        events also follow the law of large numbers and that there were in fact
        mathematical structures that could be used to model dependent systems
        apart from the old way of needing the ENTIRE history to do anything
        useful.
      </p>
      <p>
        We then fast forward to the 1950s, where American mathematician Richard
        Bellman looked at Markov Chains and thought "what if I add decisions and
        rewards?" This expanded the notion of Markov Chains into Markov Decision
        Processes, the mathematical foundation of reinforcement learning.
      </p>

      <p className="mb-4">
        <strong id="mp" className="block text-center">
          Markov Process
        </strong>
        <br />
        A quick note, Markov created the Markov Chain, described in the history
        section above, which deals ONLY with discrete (countable) time. A
        "Markov Process", is a more general term that can deal with both
        discrete or continuous time, so all Markov Chains are Markov Processes,
        but not all Markov Processes are Markov Chains. Ok, enough history, now
        back to mathematics. One of the most central ideas of a Markov Process
        is that it follows the "Markov Property", which says that the future
        depends only on the present, not on the entire past.
        <br />
        <br />
        <span className="block font-mono text-center">
          P(S<sub>t+1</sub> | S<sub>t</sub>) = P(S<sub>t+1</sub> | S<sub>1</sub>
          , S<sub>2</sub>, ‚Ä¶ , S<sub>t</sub>)
        </span>
        <br />
        In normal human language, this just means: the probability of what
        happens in the next state: P(S<sub>t+1</sub>), given the current state:
        | S<sub>t</sub>, is the same as the probability of what happens next:
        P(S<sub>t+1</sub>) given the entire history: | S<sub>1</sub>, S
        <sub>2</sub>, ‚Ä¶ , S<sub>t</sub>. In other words,{" "}
        <em>
          the current state was able to compress all the useful dependent
          information from all the past history and use all that compressed
          information to make probability judgements about the immediate future.
        </em>
        . Anything before that is just extra unnecessary baggage.
      </p>

      <p className="mb-4">
        The two fundamental objects in any Markov Process are:
        <br />
        1) The state: a compression of all important dependent information in
        the history up to that point (think of the weather at any particular
        point in time during the day)
        <br />
        2) The transition probability: represented as a matrix for discrete
        states or a probability distribution for continuous states, that tells
        you the probability of going from one state to the next (Think of the
        likelihood it will continue being cloudy at any point during the day)
        <br />
        <br />
        Formally, a <strong>Markov Process</strong> is defined as a tuple (S, P)
        where:
        <br />
        <br />‚Ä¢ <strong>S</strong> is a (finite or infinite) set of states
        <br />‚Ä¢ <strong>P</strong> is the state transition probability
        <br />
        <br />
        The state transition probability function is formally written as:
        <br />
        <br />
        <span className="block font-mono text-center">
          P<sub>ss'</sub> = P[S<sub>t+1</sub> = s' | S<sub>t</sub> = s]
        </span>
        <br />
        This notation means: "the probability of transitioning TO state s'
        (where you'll be next) GIVEN that you're currently in state s."
      </p>
      <br />

      <p className="mb-4">
        In chess, the idea of a state (board position) is very intuitive. The
        current board position (where each piece currently is on the board) is
        the state that has the Markov Property. You don&apos;t need to know how
        the pieces got there, how many blunders were made, or how confident the
        players felt on move 12. All that matters for deciding the next move is
        the position in front of you right now. Two games that arrive at the
        same board position are, from the agent&apos;s perspective, the same
        situation. That board position is a compressed summary of the entire
        history of the game, and it contains everything the agent needs to
        decide what move (action) to make next (Yea I know technically en
        passant and a few other rules don't follow this, but you get the idea).
      </p>

      <p className="mb-4">
        For discrete states, all these transition probabilities can be organized
        into a <strong>state transition matrix</strong> ùí´. The matrix is
        structured so that:
        <br />
        <br />‚Ä¢ Each <strong>row</strong> represents a starting state (where you
        are now)
        <br />‚Ä¢ Each <strong>column</strong> represents a possible next state
        (where you could go)
        <br />‚Ä¢ Each entry P<sub>ij</sub> tells you the probability of going
        FROM state i TO state j
        <br />
        <br />
        <span className="block text-center">
          <span className="italic">from</span> ùí´ ={" "}
          <span className="italic">to</span>
          <span className="inline-block ml-4">
            [P<sub>11</sub> ... P<sub>1m</sub>]
            <br />
            <span className="ml-4">[‚ãÆ ‚ã± ‚ãÆ]</span>
            <br />
            [P<sub>n1</sub> ... P<sub>nm</sub>]
          </span>
        </span>
        <br />
        <br />A key property: each row must sum to 1, because from any given
        state, you must transition <em>somewhere</em> (even if "somewhere" means
        staying in the same state). This constraint ensures the probabilities
        are valid.
      </p>

      <p className="mb-4">
        This transition matrix gives you the complete structure of the{" "}
        <strong>Markov Process</strong>. The "Markov Process" refers to the
        entire system of how states evolve over time according to these
        transition probabilities. Once you know the transition matrix ùí´, you can
        answer questions like: "If I start in state s, what&apos;s the
        probability I'll be in state s' after 10 steps?" or "What&apos;s the
        long-term distribution of states?" The matrix encodes all the dynamics
        of the system, i.e. it tells you everything about how the process moves
        through states over time. For continuous states, a probability
        distribution (called a transition kernel) is used instead of a matrix,
        but the concept is the same.
      </p>

      <p className="mb-4">
        Here is an example to better understand the complete transition matrix:
        In a simplified weather model with three states (Sunny, Cloudy, Rainy),
        the transition matrix might look like this:
        <br />
        <br />
        <span className="block font-mono text-sm">
          From Sunny: 70% ‚Üí Sunny, 20% ‚Üí Cloudy, 10% ‚Üí Rainy
          <br />
          From Cloudy: 30% ‚Üí Sunny, 40% ‚Üí Cloudy, 30% ‚Üí Rainy
          <br />
          From Rainy: 20% ‚Üí Sunny, 30% ‚Üí Cloudy, 50% ‚Üí Rainy
        </span>
        <br />
        Each row shows the transition probabilities from one state to all
        possible next states, and together they form the complete transition
        probability matrix ùí´.
      </p>
      <p className="mb-4">
        To recap: for a Markov Process all you need are two things: (1){" "}
        <strong>states</strong> that satisfy the Markov Property, meaning each
        state compresses all the relevant dependent information from the past
        into a single snapshot of "where you are now", and (2){" "}
        <strong>transition probabilities</strong> that tell you how likely you
        are to move from any given state to any other state. That&apos;s it.
        These two ingredients completely specify the entire system. With just
        these, you can predict how the system will evolve over time, calculate
        long-term behavior, and answer probability questions about sequences of
        dependent events.
      </p>

      <p className="mb-4">
        <strong id="mrp" className="block text-center">
          Markov Reward Process
        </strong>
        <br />A Markov Process gives us the mathematical tools to handle
        sequences of dependent events, but it&apos;s missing something crucial
        for reinforcement learning: feedback from the environment, like victory
        for checkmating an opponent in chess, or defeat for being checkmated. In
        RL, we want to express the environment fully, which brings us to the
        Markov Reward Process, where the environment has states, state
        transitions, AND a reward/penalty given to us by the environment which
        depends on the state we are in. Remember, the ultimate goal in RL is for
        the agent to learn which actions get us to states that give us the
        largest total cumulative reward over a given time frame (although for
        MRPs we don't have actions incorporated yet, that will come for MDPs in
        the next section, this is just laying the groundwork), so it&apos;s
        crucial that we have a way to model environment rewards from states.
        However, environment rewards in the future aren&apos;t worth as much as
        those right now (just like how in finance a dollar today is worth more
        than a dollar tomorrow), so we use a discount factor, Œ≥ (gamma), to give
        less and less value to rewards farther away from the present state.
        These are all the ingredients we need to model our environment.
      </p>
      <p className="mb-4">
        Formally, a <strong>Markov Reward Process</strong> is defined as a tuple
        (S, P, R, Œ≥) where:
        <br />
        <br />‚Ä¢ <strong>S</strong> is a finite set of states
        <br />‚Ä¢ <strong>P</strong> is the state transition probability matrix we
        saw before, where P<sub>ss'</sub> = P[S<sub>t+1</sub> = s' | S
        <sub>t</sub> = s]
        <br />
        <br />
        This notation means: "the probability of transitioning TO state s'
        (where you'll be next) GIVEN that you're currently in state s."
        <br /> <br />‚Ä¢ <strong>R</strong> is a reward function: R<sub>s</sub> =
        E[R<sub>t+1</sub> | S<sub>t</sub> = s]
        <br />
        <br />
        This notation is a bit complex, so let&apos;s break down the reward
        function notation:{" "}
        <strong>
          R<sub>s</sub> = E[R<sub>t+1</sub> | S<sub>t</sub> = s]
        </strong>
        <br />
        <br />‚Ä¢{" "}
        <strong>
          R<sub>s</sub>
        </strong>
        : The expected reward associated with state s
        <br />‚Ä¢ <strong>E[...]</strong>: Expected value (the average value
        you'll converge to as you increase the number of samples indefinitely)
        <br />‚Ä¢{" "}
        <strong>
          R<sub>t+1</sub>
        </strong>
        : The reward received at the next time step
        <br />‚Ä¢ <strong>|</strong>: "given that" (conditional probability)
        <br />‚Ä¢{" "}
        <strong>
          S<sub>t</sub> = s
        </strong>
        : You're currently in state s at time t
        <br />
        <br />
        Putting it together: R<sub>s</sub> is the expected reward you'll receive
        at the next time step when you transition away from state s.
        <br />
        <br />
        Two important notes: First, the expectation (average) E[...] is there
        because rewards can be stochastic (random). You might not always get
        exactly the same reward from the same state. Sometimes you may get 10,
        sometimes 12. R<sub>s</sub> is simply the average reward from a state if
        you were to sample it over and over again forever.
        <br />
        <br />
        Second, notice it&apos;s R<sub>t+1</sub> (next time step) | S
        <sub>t</sub>, not R<sub>t</sub> (current time step) | S<sub>t</sub>. You
        might expect that when you're in state s at time t, you would receive
        your reward (which is conditioned on being in that state) at that same
        time: i.e. R<sub>t</sub> | S<sub>t</sub> , but you don't. You only get
        that reward in the next time step once you transition to the next state.
        <br />
        <br />
        To clear up any confusion, here is an example: Suppose we have a system
        which gives a reward of 5 for the transition from state s to state s',
        and a reward of -3 for the transition from state s' to state s''.
        <br />
        <br />
        t=0: In state s ‚Üí no reward yet
        <br />
        t=1: Transition to state s' ‚Üí receive R<sub>1</sub> = 5
        <br />
        t=2: Transition to terminal state s'' ‚Üí receive R<sub>2</sub> = -3
        <br />
        <strong>Total reward: 5 + (-3) = 2</strong>
        <br />
      </p>
      <p
        id="historical-precedent-confusing-conventions"
        className="mb-4 text-center"
      >
        <strong>Historical Precedent of Confusing Conventions</strong>
      </p>
      <p className="mb-4">
        So, why would anyone use this R<sub>t+1</sub> | S<sub>t</sub> convention
        when using the R<sub>t</sub> | S <sub>t</sub> convention seems so much
        more natural? To answer this, let&apos;s look at some parallels from
        mathematics: the history of how we came up with the names sine and
        cosine, and the history of why we use the number œÄ when dealing with
        problems involving a circle&apos;s radius. Many mathematical conventions
        persist not because they're the most intuitive, but because of
        historical precedent.
      </p>
      <p className="mb-4">
        First, let&apos;s look at how we got the names sine and cosine (from the
        Latin sinus meaning pocket) for trigonometry when they seem to have no
        relation at all to the unit circle or length in general.
        <br />
        A good summary can be found here:
        <br />
        <br />
        <Link
          href="https://www.youtube.com/watch?v=AzVL432lEWA"
          target="_blank"
          className="text-blue-600 underline hover:text-blue-400"
        >
          Why sin and cos don&apos;t mean anything
        </Link>
        <br />
        <br />
        The story of how "sine" got its name involves a fascinating series of
        mistranslations across cultures. It began with Indian mathematicians
        around the 5th-6th century AD (like Aryabhata) who used the Sanskrit
        word <strong>jyƒÅ</strong> (‡§ú‡•ç‡§Ø‡§æ), meaning "bowstring", to describe what
        we now call the sine function. This made sense geometrically because the
        arc of a circle looks like a bow, and the chord (or half-chord)
        connecting its endpoints looks like the bowstring as shown here;
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/sine-arrow.png"
          alt="Intuition behind original name for sine as bowstring"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        <br />A common synonym of jyƒÅ was, <em>jƒ´v√°</em>, which is the word that
        ultimately ended up being used in India. When 8th century Persian and
        Arab scholars translated these Indian mathematical texts, they
        transliterated jƒ´v√° into Arabic as <em>jiba</em> (ÿ¨Ÿäÿ®). Arabic is
        written without short vowels, so <em>jiba</em> was written simply as the
        consonants "jb" (ÿ¨ÿ®).
        <br />
        <br />
        In the 12th century, when European scholars translated Muhammad ibn Musa
        al-Khwarizmi&apos;s famous work, al-KitƒÅb al-Mukhta·π£ar fƒ´ ·∏§isƒÅb al-Jabr
        wal-MuqƒÅbalah (which is where we get algebra [al-Jabr] from), into
        Latin, they thought that jb referred to the arabic word jayb (ÿ¨Ÿäÿ®),
        which meant pocket, and so translated it as the Latin word{" "}
        <em>sinus</em>. Finally, in 1593, in his only known publication,
        Horologiographia, English mathematician Thomas Fale converted the Latin{" "}
        <em>sinus</em> into English as "sine", and that&apos;s the word we still
        use today, instead of using the words bowstring and co-bowstring (which
        make much more sense geometrically), because of centuries of convention.
      </p>
      <p className="mb-4">
        Second, let&apos;s look at how the ratio of a circumference to its
        diameter, œÄ (pi) = 3.14..., became the number we use when dealing with
        problems involving a radius instead of œÑ (tau) = 6.28..., which is the
        ratio of a circumference to its radius.
      </p>
      <p className="mb-4">
        <br />
        There&apos;s an ongoing debate in mathematics about whether we should
        use œÄ = 3.14... or œÑ (tau) = 6.28... as our fundamental circle constant.
        Many mathematicians argue that œÑ makes more sense because it represents
        the ratio of a circle&apos;s circumference to its <em>radius</em> (C =
        œÑr [as oppused to the usual C = 2œÄr]), which would make many formulas
        cleaner. A full unit circle would simply be œÑ radians instead of 2œÄ
        radians, a quarter unit circle would be œÑ/4 instead of œÄ/2, and so on.
        <br />
        <br />
        So why do we use œÄ = 3.14... universally instead? The answer lies with
        the greatest mathematician who ever lived, Leonhard Euler. Euler used
        the greek letter œÄ (pi) simply to stand for perimeter,{" "}
        <strong>
          but let its value vary depending on whether he was working on a
          problem dealing with diameter or with radius.
        </strong>
        <br />
        <br />
        When Euler was working with problems involving <strong>diameter</strong>
        , he set œÄ equal to 3.14... (circumference to diameter ratio,). When he
        was working with problems involving <strong>radius</strong>, he set œÄ
        equal to 6.28... (circumference to radius ratio). This makes perfect
        sense, use whichever constant is most convenient for the problem at
        hand! In different works throughout the 1720s-1740s, you can find him
        using both values depending on the mathematical context.
        <br />
        <br />
        Then in 1748, Euler published his famous calculus book{" "}
        <em>Introductio in Analysin Infinitorum</em> (Introduction to the
        Analysis of the Infinite). In the book, he happened to be working with
        diameter based formulas, so he used œÄ = 3.14... This textbook became one
        of the most important mathematical works of the 18th century, widely
        read and studied across Europe.
        <br />
        <br />
        However, instead of following Euler&apos;s pragmatic approach of
        switching between values based on context, later mathematicians simply
        adopted the value for œÄ as 3.14 regardless of the mathematical context
        because that was the value they had seen Euler use in this book. They
        stopped adjusting based on whether they were working with radius or
        diameter. That&apos;s why, instead of having a unit circle like this:
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/unit-circle-tau.png"
          alt="Unit Circle using tau showing that the values fit the circle"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        <br />
        where going around 1/12th of the circle is œÑ/12, 1/4 of the circle is
        œÑ/4, 3/8ths of the circle is 3œÑ/8, etc, we have this:
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/unit-circle-pi.png"
          alt="Unit Circle using pi showing that the values are too large for the circle"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        <br />
        along with many other confusing and inelegant complications when using a
        diameter based constant for radius based problems.
        <br />
        <br />
        Euler&apos;s flexible approach was, of course, the sensible approach,
        but we continue to have a standardized value because of centuries of
        convention.
      </p>
      <p className="mb-4">
        I think you can see where I&apos;m going with this. There are many areas
        of mathematics where notation is used that is less intuitive for
        learners simply because of convention. The convention of giving the
        reward at timestep t+1, even though its based of the state at time t, is
        now standard in RL for similar historical reasons: i.e. because it
        appeared in the most influential RL textbook (Reinforcement Learning: An
        Introduction by Sutton & Barto) in 1998 and everyone adopted it.
      </p>

      <p id="mrp-continued" className="mb-4 text-center">
        <strong>Markov Reward Process Continued</strong>
      </p>
      <p className="mb-4">
        Now we come to the final part in the Markov Reward Process, which is the
        discount factor.
        <br /> <br />‚Ä¢ <strong>Œ≥</strong> (gamma) is a discount factor, where Œ≥
        ‚àà [0,1] (gamma is just a number between 0 and 1).
        <br />
        <br />
        The discount factor, Œ≥, determines how much we value future rewards
        relative to immediate rewards. When we calculate the total sum of
        rewards (the return) from a given state, G<sub>t</sub>, we don&apos;t
        just add up all future rewards equally (unless we set Œ≥ equal to 1), we
        discount them based on how far in the future they occur:
        <br />
        <br />
        <span className="block font-mono text-center">
          G<sub>t</sub> = R<sub>t+1</sub> + Œ≥R<sub>t+2</sub> + Œ≥¬≤R
          <sub>t+3</sub> + ...
        </span>
        <br />
        <br />
        <strong>Why discount at all?</strong> There are several important
        reasons:
        <br />
        <br />
        1) <strong>Uncertainty about the future</strong>: For complex real world
        problems, it&apos;s very difficult to make any algorithm that can
        accurately predict the far future equally as well as the immediate
        future. Rewards far in the future are less certain to actually happen. A
        reward you might get in 100 steps is less reliable than one you&apos;ll
        get in 50 steps which is less reliable than one you&apos;ll get in one
        step, so they should each be weighted less.
        <br />
        <br />
        2) <strong>Mathematical convenience</strong>: Discounting ensures that
        infinite sums of rewards converge to a finite number, making the math
        work better. Without Œ≥ (unless you set Œ≥ = 1), an infinite episode would
        have infinite return, making the math more difficult to work with.
        <br />
        <br />
        3) <strong>Models real preferences</strong>: In many scenarios, getting
        something now is genuinely more valuable than getting it later. This is
        like the time value of money in finance, a dollar today is worth more
        than a dollar tomorrow because you could invest it and earn interest.
        <br />
        <br />
        <strong>What does Œ≥ mean in practice?</strong>
        <br />
        If <strong>Œ≥ = 0</strong>: The agent only cares about immediate rewards,
        completely ignoring the future (myopic)
        <br />
        If <strong>Œ≥ = 1</strong>: All future rewards count equally, no matter
        how far away (far-sighted, only works for finite episodes)
        <br />
        If <strong>Œ≥ = 0.5</strong>: A reward 4 steps away is worth (0.5)
        <sup>4</sup> ‚âà 0.0625 of what that same reward would be worth on this
        step
        <br />
        <br />
        The choice of Œ≥ is a design decision set by humans, which significantly
        affects what the agent learns to do. A higher Œ≥ makes the agent more
        patient and considerate of long-term consequences, while a lower Œ≥ does
        the opposite.
        <br />
        <br />
      </p>
      <p className="mb-4">
        Now that we understand how the discount factor shapes our view of future
        rewards, let&apos;s introduce the <strong>value function</strong>. While
        the reward function tells us the immediate reward from being in state s,
        the value function tells us something more useful: the expected total
        return from state s onwards. It&apos;s the difference between asking
        "what do I get right now?" versus "what&apos;s the total payoff I can
        expect from here?"
        <br />
        <br />
        (Note: We&apos;re introducing the value function for Markov Reward
        Processes first as a stepping stone. The value function for MDPs, which
        we&apos;ll see later, is policy-dependent and slightly different, but
        the core concept is the same.)
        <br />
        <br />
        Formally, the state value function v(s) of an MRP is the expected return
        (total sum of all discounted rewards) starting from state s:
        <br />
        <br />
        <span className="block font-mono text-center">
          v(s) = E[G<sub>t</sub> | S<sub>t</sub> = s]
        </span>
        <br />
        Breaking this down: we're taking the expectation (average) of the return
        G<sub>t</sub> (remember, that&apos;s the sum of all discounted future
        rewards: R<sub>t+1</sub> + Œ≥R<sub>t+2</sub> + Œ≥¬≤R
        <sub>t+3</sub> + ...) given that we start in state s.
        <br />
        <br />
        For example, imagine a state that gives you a small immediate reward of
        +1, but always transitions to terrible states that give -10 rewards.
        That state has a high immediate reward, but a low value function.
        Conversely, if a state gives you 0 immediate reward, but leads to a
        sequence of highly rewarding states, that state would have a high value
        function despite its low immediate reward. The value function captures
        this "looking ahead" perspective.
      </p>
      <p className="mb-4">
        <strong className="block text-center">
          Return vs Expected Return: What&apos;s the Difference?
        </strong>
        <br />
        This is a subtle but crucial distinction. Let&apos;s break down why G
        <sub>t</sub> and v(s) look different mathematically.
        <br />
        <br />
        <strong>
          G<sub>t</sub> (the return)
        </strong>{" "}
        describes what happens in ONE specific trajectory:
        <br />
        <br />
        <span className="block font-mono text-center">
          G<sub>t</sub> = R<sub>t+1</sub> + Œ≥R<sub>t+2</sub> + Œ≥¬≤R
          <sub>t+3</sub> + ...
        </span>
        <br />
        Notice the R&apos;s are the ACTUAL REWARDS you receive as you move
        through the MRP. No expectation needed because this is describing what
        literally happened: "I got reward R<sub>t+1</sub>, then R<sub>t+2</sub>,
        then R<sub>t+3</sub>..." Each specific return will probably be different
        every sample, but over MANY samples, the average of these returns will
        converge to a stable value due to the law of large numbers.
        <br />
        <br />
        <strong>v(s) (the value function)</strong> describes what you EXPECT
        across ALL possible trajectories:
        <br />
        <br />
        <span className="block font-mono text-center">
          v(s) = E[G<sub>t</sub> | S<sub>t</sub> = s]
        </span>
        <br />
        Now we need the expectation E[...] because from state s, many different
        things can happen:
      </p>
      <ul className="list-disc ml-8 mb-4">
        <li>
          The transition probabilities might send you to different next states
        </li>
        <li>
          The rewards might be stochastic (sometimes +5, sometimes +3 from the
          same state)
        </li>
        <li>
          Each possible path forward has its own G<sub>t</sub>
        </li>
      </ul>
      <p className="mb-4">
        But overall, the expected value (the average value you will get if you
        did an infinite amount of random samples) is what v(s) will be.
      </p>
      <p className="mb-4">
        <strong>Example:</strong> You're in state s. From s, there&apos;s a 50%
        chance to go to state A (which gives reward +10) and 50% chance to go to
        state B (which gives reward +2). Let&apos;s say both A and B are
        terminal states.
        <br />
        <br />
        <strong>Trajectory 1:</strong> s ‚Üí A
        <br />G<sub>t</sub> = 10 (this is the actual return for this specific
        path)
        <br />
        <br />
        <strong>Trajectory 2:</strong> s ‚Üí B
        <br />G<sub>t</sub> = 2 (this is the actual return for this specific
        path)
        <br />
        <br />
        <strong>Value function:</strong>
        <br />
        v(s) = E[G<sub>t</sub> | S<sub>t</sub> = s] = (0.5 √ó 10) + (0.5 √ó 2) =
        (5) + (1) = 6 (this is the expected return from this state)
        <br />
        <br />
        <strong>Summary of Notation</strong>
      </p>
      <ul className="list-disc ml-8 mb-4">
        <li>
          G<sub>t</sub>: No expectation operator ‚Üí describes ONE trajectory,
          uses actual rewards received
        </li>
        <li>
          v(s): Has expectation operator E[...] ‚Üí averages over ALL possible
          trajectories
        </li>
      </ul>

      <p className="mb-4">
        To recap, a Markov Reward Process gives us a complete mathematical
        description of an environment by extending Markov Processes with rewards
        and a discount factor. It&apos;s defined by the tuple (S, P, R, Œ≥) where
        S captures all possible states the environment can be in, P describes
        how the environment transitions between states, R specifies the expected
        feedback (rewards or penalties) the environment will give us, and Œ≥
        determines how we weight rewards that occur farther and farther into the
        future. Together, these four components fully characterize the
        environment&apos;s structure. The value function v(s) tells us the
        expected total discounted return from state s, not just the immediate
        reward (which is what the reward function gives us), but the long-term
        value accounting for all future rewards. Later, Markov Reward Processes
        will be extended into Markov Decision Processes, where an agent can take
        actions to influence which states it enters. This is the core idea of
        RL: building algorithms that allow agents to learn which actions
        maximize their chances of being in states with the highest expected
        return.
        <br />
      </p>
      <p className="mb-4">
        <strong
          id="bellman-equation-introduction"
          className="block text-center"
        >
          Introduction to the Bellman Equation for MRPs
        </strong>
      </p>
      <p className="mb-4">
        This will only be a brief introduction to the Bellman Equation, which
        will be explained in much more detail in the next section about dynamic
        programming. This is one of the most important and fundamental concepts
        to understand in RL. The key insight is that instead of computing the
        value function by summing up all future discounted rewards directly, we
        can use recursion. Here&apos;s how it works formally:
        <br />
        <br />
        <span className="block font-mono text-center">
          v(s) = E[G<sub>t</sub> | S<sub>t</sub> = s]
        </span>
        <br />
        <span className="block font-mono text-center">
          = E[R<sub>t+1</sub> + Œ≥R<sub>t+2</sub> + Œ≥¬≤R<sub>t+3</sub> + ... | S
          <sub>t</sub> = s]
        </span>
        <br />
        <span className="block font-mono text-center">
          = E[R<sub>t+1</sub> + Œ≥(R<sub>t+2</sub> + Œ≥R<sub>t+3</sub> + ...) | S
          <sub>t</sub> = s]
        </span>
        <br />
        <span className="block font-mono text-center">
          = E[R<sub>t+1</sub> + Œ≥G<sub>t+1</sub> | S<sub>t</sub> = s]
        </span>
        <br />
        <span className="block font-mono text-center">
          = E[R<sub>t+1</sub> + Œ≥v(S<sub>t+1</sub>) | S<sub>t</sub> = s]
        </span>
        <br />
        <br />
        Notice how we factored out Œ≥ from the infinite sum R<sub>t+2</sub> + Œ≥R
        <sub>t+3</sub> + Œ≥¬≤R<sub>t+4</sub> + ..., which is exactly G
        <sub>t+1</sub> (the return starting from time t+1). By definition, the
        expected value of G<sub>t+1</sub> starting from state S<sub>t+1</sub> is
        just the value function of the next state v(S
        <sub>t+1</sub>). Thus, instead of computing a very long sum of rewards,
        we only need to look one step ahead and do two things:
        <br />
        <br />
        1) Add the immediate reward R<sub>t+1</sub>
        <br />
        2) Add the discounted value Œ≥v(S<sub>t+1</sub>),
        <br />
        trusting that the value function at the next state has already done the
        rest of the work for us.
      </p>

      <p className="mb-4">
        <Image
          src="/images/reinforcement-learning/mdp/bellman-mrp-model.png"
          alt="Bellman equation diagram showing state s transitioning to next states"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        The diagram above illustrates this one-step lookahead perspective. From
        state s, we can transition to various next states s&apos; with different
        probabilities. The Bellman Equation can be written explicitly in terms
        of these transition probabilities:
        <br />
        <br />
        <span className="block font-mono text-center">
          v(s) = R<sub>s</sub> + Œ≥ Œ£<sub>s&apos;</sub> P<sub>ss&apos;</sub>
          v(s&apos;)
        </span>
        <br />
        <br />
        This equation says: the value of state s, (v(s)), equals the immediate
        expected reward, R<sub>s</sub>, plus a discounted (Œ≥) sum over all
        possible next states (v(s&apos;)) weighted by their probability of
        occuring (P<sub>ss&apos;</sub>). Here&apos;s how to compute it step by
        step:
        <br />
        <br />
        1) Look at the first possible next state, s&apos;, you can transition to
        from s
        <br />
        2) Find its discounted value Œ≥v(s&apos;)
        <br />
        3) Multiply it by P<sub>ss&apos;</sub>, the probability of transitioning
        from s to s&apos; (given in the transition matrix P)
        <br />
        4) Repeat steps 1-3 for every other possible next state s&apos;
        <br />
        5) Add all these weighted values together
        <br />
        6) Finally, add the immediate expected reward R<sub>s</sub> you get from
        being in state s
      </p>
      <p className="mb-4">
        The Bellman Equation becomes even more powerful when we express it in
        matrix form, which allows us to solve for all state values
        simultaneously using linear algebra. Instead of computing v(s) for each
        individual state separately, we can organize all the state values into a
        column vector
        <strong> v</strong>, where each entry corresponds to one state&apos;s
        value. We multiply v by the identity matrix. Similarly, we organize all
        the immediate rewards into a column vector
        <strong> R</strong>. The transition probabilities, which tell us how
        likely we are to move from any state to any other state, form a matrix{" "}
        <strong>P</strong>. With this setup, the Bellman Equation for the entire
        MRP can be written compactly as:
        <br />
        <br />
        <span className="block font-mono text-center">
          <strong>Iv</strong> = <strong>R</strong> + Œ≥<strong>P</strong>
          <strong>v</strong>
        </span>
        <br />
        <br />
        This single matrix equation encodes the Bellman Equation for every state
        at once. The matrix multiplication <strong>Pv</strong> automatically
        computes the weighted sum of next-state values for each state (exactly
        what the summation Œ£<sub>s&apos;</sub> P<sub>ss&apos;</sub>v(s&apos;)
        did), and adding <strong>R</strong> gives us the immediate rewards. This
        formulation is crucial because it transforms our problem into a system
        of linear equations that we can solve directly in the following way:
        <span className="block font-mono text-center mt-4">
          <strong>Iv</strong> - Œ≥<strong>Pv</strong> = <strong>R</strong>
        </span>
        <br />
        <span className="block font-mono text-center">
          (<strong>I</strong> - Œ≥<strong>P</strong>)<strong>v</strong> ={" "}
          <strong>R</strong>
        </span>
        <br />
        <span className="block font-mono text-center">
          <strong>v</strong> = (<strong>I</strong> - Œ≥<strong>P</strong>)
          <sup>-1</sup> <strong>R</strong>
        </span>
        <br />
        <br />
      </p>
      <p className="mb-4">
        Matrix inversion is computationally expensive for large state spaces
        (O(n¬≥) complexity for n states), but more efficient iterative methods
        will be discussed in later sections.
      </p>
      <p className="mb-4">
        <strong id="mdp" className="block text-center">
          Markov Decision Process
        </strong>
      </p>
      <p className="mb-4">
        Now we are ready to talk about the full mathematical structure that can
        describe reinforcement learning, the Markov Decision Process. With a
        Markov Reward Process we could fully describe the environment of RL, but
        there was still nothing to describe the agent. An agent doesn't just
        exist in an environment, it can actively choose to take actions that
        affect its probability of ending up in a particular state. We now have
        two separate events that affect the probability of going to s' from s:
        <br />
        <br />
        1) The action the agent takes while in state s
        <br />
        2) The environment&apos;s dynamics (which were discussed in the Markov
        Reward Process section)
      </p>
      <p className="mb-4">
        Formally, a <strong>Markov Decision Process</strong> is defined as a
        tuple (S, A, P, R, Œ≥) where:
        <br />
        <br />‚Ä¢ <strong>S</strong> is a finite set of states
        <br />‚Ä¢ <strong>A</strong> is a finite set of actions the agent can take
        <br />‚Ä¢ <strong>P</strong> is the state transition probability matrix,
        now action-dependent: P<sup>a</sup>
        <sub>ss&apos;</sub> = P[S<sub>t+1</sub> = s&apos; | S<sub>t</sub> = s, A
        <sub>t</sub> = a]
        <br />
        <br />
        This reads as: "the probability of transitioning to state s&apos; given
        that you&apos;re currently in state s AND you take action a"
        <br />
        <br />‚Ä¢ <strong>R</strong> is the reward function, also now
        action-dependent: R<sup>a</sup>
        <sub>s</sub> = E[R<sub>t+1</sub> | S<sub>t</sub> = s, A<sub>t</sub> = a]
        <br />
        <br />
        This reads as: "the expected reward you receive given you&apos;re in
        state s AND you take action a"
        <br />
        <br />‚Ä¢ <strong>Œ≥</strong> is the discount factor, Œ≥ ‚àà [0,1], same as
        before
        <br />
        <br />
        The key difference from MRPs is that both P and R now depend on the
        action taken, giving the agent control over transitions and rewards. In
        MRPs, P was entirely determined by the environment dynamics, but now
        it&apos;s a combination of the agent&apos;s action AND the
        environment&apos;s transition dynamics.
        <br />
        <br />
        For example, imagine a robot serving food at an outdoor cafe. If the
        customer is to the right of the robot, the robot&apos;s action to move
        itself to the right greatly increases the probability of reaching the
        customer, but it&apos;s not guaranteed. A strong wind might push the
        robot to the left instead. The new state depends on both the
        agent&apos;s chosen action and environmental factors like wind.
        <br />
        <br />
        You can think of there being a separate set of transition probabilities
        for each possible action in each state, (unlike MRPs where there was
        only one set of transition probabilities per state ). Similarly, rewards
        now depend on both state AND action as well. Taking different actions in
        the same state can yield different expected rewards (unlike MRPs where
        the reward only depended on the current state).
      </p>

      <p className="mb-4">
        In order to talk about actions, I need to define what it means in an RL
        context for an agent to decide to take an action. In order to do that,
        we need to learn about what a policy is. A policy fully defines the
        behavior of an agent, telling the agent what action it will take in a
        given state (or the probabilities for a set of available actions it can
        take in a given state). In chess, this is the part of the agent that
        looks at the board and decides, "In this position, I'm going to move the
        knight to this square," or "I'm going to do something bold and probably
        regret it."
      </p>

      <p className="mb-4">
        Formally, a policy œÄ is a distribution over actions given states:
        <br />
        <br />
        <span className="block font-mono text-center">
          œÄ(a|s) = P[A<sub>t</sub> = a | S<sub>t</sub> = s]
        </span>
        <br />
        <br />
        This reads as: "the probability that the agent takes action a given that
        it&apos;s currently in state s." There are two types of policies:
        deterministic policies and stochastic policies. A
        <strong> deterministic policy</strong> always chooses the same action in
        a given state, there is no randomness. œÄ(a|s) equals 1 for one specific
        action and 0 for all others. A <strong>stochastic policy</strong>{" "}
        assigns probabilities to different actions in each state, for example,
        œÄ(a‚ÇÅ|s) = 0.7 and œÄ(a‚ÇÇ|s) = 0.3 means there&apos;s a 70% chance of
        taking action a‚ÇÅ and a 30% chance of taking action a‚ÇÇ when in state s.
        The optimal policy is what we ultimately want to learn in RL, i.e. the
        best actions an agent can take to maximize its expected total reward.
      </p>

      <p className="mb-4">
        <strong>An important note:</strong> Once we fix a policy œÄ, the MDP
        behaves exactly like an MRP. The transition probabilities become
        policy-specific: P<sup>œÄ</sup>
        <sub>ss'</sub> = Œ£<sub>a</sub> œÄ(a|s)P<sup>a</sup>
        <sub>ss'</sub>, and the rewards become policy-specific: R<sup>œÄ</sup>
        <sub>s</sub> = Œ£<sub>a</sub> œÄ(a|s)R<sup>a</sup>
        <sub>s</sub>.
        <br />
        <br />
        In other words, when the agent commits to following a specific policy,
        we can collapse the state-action dynamics back into state only dynamics.
        The policy tells us the probability of each action from each state, so
        we can compute weighted averages over actions to get a single set of
        transition probabilities and expected rewards for each state (instead of
        needing them for each action in a state). This allows us to use the MRP
        Bellman equation we saw earlier for evaluating a specific policy,
        because once you fix a policy, an MDP can be reduced to an MRP (you can
        derive values from states only, instead of state-action pairs). However,
        usually we don't want to compress MDPs into MRPs because we want to know
        the state-action value pairs instead of just the state values. The
        reasons why will be explained later.
      </p>

      <p className="mb-4">
        If the policy is the agent&apos;s ‚Äúwhat should I do in this state?‚Äù
        mechanism, then the value function is its ‚Äúhow good is this state?‚Äù
        mechanism. The value function is the agent&apos;s prediction of how much
        total reward it expects to collect in the future, starting from a
        particular state. I already discussed value functions for MRPs in the
        MRP section, but there was no way for the agent to make any decisions to
        affect the likelihood of transitioning to different states (thereby
        affecting overall reward). With MDPs, the value function is now entirely
        dependent on the agent&apos;s policy because your future reward given on
        a state DEPENDS on what moves you are likely to make in future states.
        Bobby Fischer would likely have a very different value function in a
        given chess position than someone who just learned how to play chess
        because Fischer is very likely to make different moves than the person
        who just learned chess. Therefore instead of v(s) as we wrote for the
        MRP, we now write v<sub>œÄ</sub>(s), where œÄ is the policy.
      </p>

      <p className="mb-4">
        MDPs have two value functions:
        <br />
        <br />
        <strong>
          1. State-Value Function v<sub>œÄ</sub>(s)
        </strong>
        <br />
        <br />
        <span className="block font-mono text-center">
          v<sub>œÄ</sub>(s) = E<sub>œÄ</sub>[G<sub>t</sub> | S<sub>t</sub> = s]
        </span>
        <br />
        <br />
        The state-value function tells us the expected return (total discounted
        reward) starting from state s and then following policy œÄ.
        <br />
        <br />
        <strong>Chess analogy:</strong> Imagine you&apos;re looking at a
        specific chess position. The state-value function answers: "If I follow
        my usual strategy (policy) from this position onwards, how good is my
        expected outcome?" A position where you&apos;re up a queen might have a
        high value because following a reasonable strategy from there typically
        leads to winning. A position where you&apos;re down a queen has a very
        low value because your strategy probably can&apos;t save you.
        <br />
        <br />
        <strong>
          2. Action-Value Function q<sub>œÄ</sub>(s,a)
        </strong>
        <br />
        <br />
        <span className="block font-mono text-center">
          q<sub>œÄ</sub>(s,a) = E<sub>œÄ</sub>[G<sub>t</sub> | S<sub>t</sub> = s,
          A<sub>t</sub> = a]
        </span>
        <br />
        <br />
        The action-value function tells us the expected return starting from
        state s, taking action a, and then following policy œÄ afterwards.
        <br />
        <br />
        <strong>Chess analogy:</strong> Instead of just evaluating your
        position, you're evaluating a specific move from that position. The
        action-value function answers: "If I make this particular move (like
        knight to c3) and then follow my usual strategy afterwards, what&apos;s
        my expected outcome?" You can compare this to another move from the same
        position (like bishop to g5) to see which is better.
        <br />
        <br />
        <strong>Key difference:</strong> v<sub>œÄ</sub>(s) gives you one number
        per state, the average value across all actions you might take (weighted
        by your policy). But the action-value function q<sub>œÄ</sub>(s,a) gives
        you a separate value for each individual action available in that state.
        You might be in a strong position overall (high v<sub>œÄ</sub>(s)), but
        one specific move could be disastrous (q<sub>œÄ</sub>(s,a<sub>1</sub>)
        for that action) while another could be brilliant (q<sub>œÄ</sub> (s,a
        <sub>2</sub>) for a different action).
      </p>

      <p id="bellman-expectation-equation" className="mb-4">
        <strong className="block text-center mb-4">
          Bellman Expectation Equation for MDPs
        </strong>
        Just like in MRPs, the value functions in MDPs can be decomposed
        recursively using Bellman Equations. However, since MDPs involve
        policies and actions, we now have two types of Bellman Equations for
        MDPs: the <strong>Bellman Expectation Equation</strong> and the{" "}
        <strong>Bellman Optimality Equation</strong>. The Bellman Expectation
        Equation tells us how to evaluate a specific policy œÄ, while the Bellman
        Optimality Equation tells us how to find the best possible policy.
        <br />
        <br />
        <strong>For the state-value function:</strong>
        <br />
        <br />
        <span className="block font-mono text-center">
          v<sub>œÄ</sub>(s) = E<sub>œÄ</sub>[R<sub>t+1</sub> + Œ≥v
          <sub>œÄ</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]
        </span>
        <br />
        <br />
        This says: the value of being in state s under policy œÄ equals the
        expected immediate reward plus the discounted value of wherever you end
        up next, when following policy œÄ.
        <br />
        <br />
        <strong>Chess analogy:</strong> The value of your current board position
        equals: the immediate reward you get (maybe capturing a piece) plus the
        discounted value of the position after your move, assuming you continue
        playing according to your strategy. If your strategy says "70% of the
        time move the knight, 30% of the time move the bishop," then the value
        is the weighted average over these possibilities.
        <br />
        <br />
        <strong>For the action-value function:</strong>
        <br />
        <br />
        <span className="block font-mono text-center">
          q<sub>œÄ</sub>(s, a) = E<sub>œÄ</sub>[R<sub>t+1</sub> + Œ≥q
          <sub>œÄ</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>) | S<sub>t</sub> = s, A
          <sub>t</sub> = a]
        </span>
        <br />
        <br />
        This says: the value of taking action a in state s under policy œÄ equals
        the expected immediate reward from taking that action plus the
        discounted action-value of the next state-action pair, when following
        policy œÄ.
        <br />
        <br />
        <strong>Chess analogy:</strong> The value of moving your knight to c3
        equals: any immediate reward from that move (like attacking the queen)
        plus the discounted value of whatever move your strategy tells you to
        make in the resulting position. Notice we&apos;re looking ahead to the
        value of the <em>next move</em> (q at t+1) in the next position, not
        solely the next position (v at t+1).
        <br />
        <br />
        <strong>Important note:</strong> These are called "expectation"
        equations because we&apos;re taking expectations under a{" "}
        <em>specific policy</em> œÄ. They tell us "how good is this state (or
        state-action pair) under this current strategy I'm using? (which might
        not be the best possible strategy that exists)" Soon, we&apos;ll learn
        about the <strong>Bellman Optimality Equation</strong>, which tells us
        "what&apos;s the value under the <em>best possible</em> strategy?"
      </p>

      <p className="mb-4">
        <strong className="block text-center mb-4">
          Visualizing the Bellman Expectation Equations
        </strong>
        Let&apos;s visualize how these recursive relationships work using tree
        diagrams. These diagrams show the "backup" operations, which is how we
        compute values by looking one step ahead and then "backing up" that
        information to the state we're trying to compute the value of.
        <br />
        <br />
        <strong>State-Value to Action-Value:</strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-vs.png"
          alt="Bellman expectation equation showing state-value decomposed into action-values"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        The first diagram shows how to compute v<sub>œÄ</sub>(s) from
        action-values. Starting from state s (top), we consider all possible
        actions a that our policy might choose (shown as dark circles at the
        bottom). The value of state s is the weighted average of these
        action-values, where the weights are the policy probabilities œÄ(a|s):
        <br />
        <br />
        <span className="block font-mono text-center">
          v<sub>œÄ</sub>(s) = Œ£<sub>a</sub> œÄ(a|s)q<sub>œÄ</sub>(s, a)
        </span>
        <br />
        <br />
        <strong>Chess analogy:</strong> You&apos;re evaluating your current
        position. Your strategy might say "40% chance move the knight, 30% move
        the bishop, 30% castle." The value of your position is the weighted
        average: 0.4 √ó (value of knight move) + 0.3 √ó (value of bishop move) +
        0.3 √ó (value of castling).
        <br />
        <br />
        <strong>Action-Value to State-Value:</strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-qs.png"
          alt="Bellman expectation equation showing action-value decomposed into next state-values"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        The second diagram shows how to compute q<sub>œÄ</sub>(s,a) from
        state-values. Starting from state-action pair (s,a) at the top (dark
        circle‚Äîwe&apos;ve committed to action a), the environment transitions us
        to various possible next states s&apos; (light circles at bottom). The
        action-value equals the immediate reward plus the discounted sum of
        next-state values, weighted by transition probabilities:
        <br />
        <br />
        <span className="block font-mono text-center">
          q<sub>œÄ</sub>(s, a) = R<sub>s</sub>
          <sup>a</sup> + Œ≥ Œ£<sub>s&apos;</sub> P<sub>ss&apos;</sub>
          <sup>a</sup>v<sub>œÄ</sub>(s&apos;)
        </span>
        <br />
        <br />
        <strong>Chess analogy:</strong> You&apos;re evaluating moving your
        knight to c3. After this move, your opponent has several possible
        responses (different s&apos;). The value of your knight move equals any
        immediate reward (threatening pieces) plus the average value of the
        positions you&apos;ll face, weighted by how likely your opponent is to
        respond by moving their piece to create said positions.
        <br />
        <br />
        <strong>The key insight:</strong> These two equations work together! The
        first expresses v in terms of q, and the second expresses q in terms of
        v. Together, they form a recursive system that fully characterizes the
        value functions under policy œÄ. Notice the difference in the circles:
        dark circles represent decision points (where the agent or policy
        chooses), light circles represent chance nodes (where the environment
        decides based on transition probabilities).
      </p>

      <p className="mb-4">
        <strong className="block text-center mb-4">
          The Complete Bellman Expectation Equations: Two Step Backup
        </strong>
        Now let&apos;s see what happens when we combine both relationships into
        complete recursive equations. By substituting one equation into the
        other, we can express each value function entirely in terms of itself!
        <br />
        <br />
        <strong>State-Value Full Backup:</strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-v-q-together.png"
          alt="Complete Bellman expectation equation for state-value showing two-step lookahead"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        Starting from state s (top light circle), we first consider all possible
        actions our policy might choose (middle dark circles), then for each
        action, we look at all possible next states s&apos; (bottom light
        circles). This gives us:
        <br />
        <br />
        <span className="block font-mono text-center">
          v<sub>œÄ</sub>(s) = Œ£<sub>a</sub> œÄ(a|s)(R<sub>s</sub>
          <sup>a</sup> + Œ≥ Œ£<sub>s&apos;</sub> P<sub>ss&apos;</sub>
          <sup>a</sup>v<sub>œÄ</sub>(s&apos;))
        </span>
        <br />
        <br />
        <strong>Chess analogy:</strong> To evaluate your position, you consider
        all moves your strategy might make (weighted by probability). For each
        move, you look at all positions your opponent might create (weighted by
        likelihood). The value is: (immediate reward from moves) + (average
        value of resulting positions). This is a two-step lookahead: position ‚Üí
        your possible moves ‚Üí opponent&apos;s responses.
        <br />
        <br />
        <strong>Action-Value Full Backup:</strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-q-v-together.png"
          alt="Complete Bellman expectation equation for action-value showing two-step lookahead"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <br />
        Starting from state-action pair (s,a) (top dark circle where we&apos;ve
        chosen action a), the environment transitions us to possible next states
        s&apos; (middle light circles), and then from each s&apos;, our policy
        considers possible next actions a&apos; (bottom dark circles). This
        gives us:
        <br />
        <br />
        <span className="block font-mono text-center">
          q<sub>œÄ</sub>(s, a) = R<sub>s</sub>
          <sup>a</sup> + Œ≥ Œ£<sub>s&apos;</sub> P<sub>ss&apos;</sub>
          <sup>a</sup> Œ£<sub>a&apos;</sub> œÄ(a&apos;|s&apos;)q<sub>œÄ</sub>
          (s&apos;, a&apos;)
        </span>
        <br />
        <br />
        <strong>Chess analogy:</strong> You&apos;ve decided to move your knight
        to c3. Your opponent responds, creating various positions. From each
        position, your strategy determines your next move. The value of "knight
        to c3" is: (immediate reward) + (average over opponent responses i.e.
        all the s' states, and then the value of your follow up moves in each of
        these hypothetical resulting position). This is also a two-step
        lookahead: your move ‚Üí opponent&apos;s response ‚Üí your next move.
      </p>

      <p className="mb-4">
        <strong className="block text-center mb-4">
          Comparing Policies: What Makes One Better Than Another?
        </strong>
        Before we talk about finding the <em>best</em> policy, we need to define
        what "better" even means. In reinforcement learning, we say that one
        policy œÄ is better than or equal to another policy œÄ' if its value
        function is greater than or equal to the other policy&apos;s value
        function FOR ALL STATES:
        <br />
        <br />
        <span className="block font-mono text-center">
          œÄ ‚â• œÄ' if v<sub>œÄ</sub>(s) ‚â• v<sub>œÄ'</sub>(s) for all s ‚àà S
        </span>
        <br />
        <br />
        Notice the "for all states" part, this is crucial. A policy isn't just
        better because it does well in <em>some</em> positions; it has to do at
        least as well everywhere. If there is even ONE state where the other
        policy has a higher value, then we can't say which policy is better.
        <br />
        <br />
        <strong>Chess analogy:</strong> Think of Bobby Fischer versus inferior
        chess players (Soviet Grand Masters for instance). Fischer&apos;s policy
        (his chess strategy) is better because from <em>any</em> board position
        you give him, he'll achieve an equal or better expected outcome than the
        Soviet Grand Master. It doesn't matter if the position is a complex
        middlegame, an endgame with just a few pieces, or a weird opening you've
        never seen, Fischer just plays every move better, meaning v
        <sub>Fischer</sub>
        (s) ‚â• v<sub>SovietGrandMaster</sub>(s) for every position s. There are
        no positions you can cherry pick specific positions where the Soviet
        Grand Master does better. (Yes, I know Fischer wasn't better than the
        Soviet Grand Masters in all positions, but it gets the point across)
        <br />
        <br />
        This definition of "better" leads us naturally to ask: is there a policy
        that&apos;s better than or equal to <em>all</em> other policies?
        Remarkably, the answer is yes! We call such a policy an{" "}
        <strong>optimal policy</strong>, denoted œÄ<sub>*</sub>. An optimal
        policy satisfies:
        <br />
        <br />
        <span className="block font-mono text-center">
          œÄ<sub>*</sub> ‚â• œÄ for all policies œÄ
        </span>
        <br />
        <br />
        In other words, v<sub>œÄ*</sub>(s) ‚â• v<sub>œÄ</sub>(s) for every state s
        and every other policy œÄ. This optimal policy achieves the maximum
        possible value in every state. (Note: there can be multiple optimal
        policies that are equally good, think of a game where picking up an
        object with your right hand versus your left hand both lead to the same
        outcome).
        <br />
        <br />
      </p>

      <p className="mb-4">
        <strong className="block text-center mb-4">
          Optimal Value Functions
        </strong>
        So far we&apos;ve been evaluating specific policies, "how good is this
        PARTICULAR strategy?" But the ultimate goal in RL is to find the{" "}
        <em>best possible policy</em> "what is the BEST strategy that will give
        me the most total cumulative reward". To do this, we need to be able to
        compare policy values, which brings us to optimal value functions, which
        tell us the maximum value achievable in any state or state-action pair.
        <br />
        <br />
        The <strong>optimal state-value function</strong> v<sub>*</sub>(s) is
        the maximum value function over all policies (you look at every policy
        and choose the one gives you the highest total return at every state):
        <br />
        <br />
        <span className="block font-mono text-center">
          v<sub>*</sub>(s) = max<sub>œÄ</sub> v<sub>œÄ</sub>(s)
        </span>
        <br />
        <br />
        <strong>Chess analogy:</strong> Think of Bobby Fischer&apos;s strategy
        as an optimal policy œÄ<sub>*</sub>, the best possible way to play chess.
        When Fischer follows his strategy from any position, he achieves the
        best possible outcome. The optimal state-value function v<sub>*</sub>(s)
        tells us the value of any position, if from that point forward we
        started following Fischer&apos;s strategy. So if you're up a queen, v
        <sub>*</sub>(s) is very high because even with perfect play from your
        opponent, you'll probably win. If you're down material hopelessly, v
        <sub>*</sub>(s) is low because even Fischer himself can't save that
        position.
        <br />
        <br />
        Here&apos;s the key distinction: œÄ<sub>*</sub> (the optimal policy)
        tells you <em>what moves to make</em>, while v<sub>*</sub>
        (s) (the optimal value function) tells you{" "}
        <em>how good the resulting position will be</em> when you follow that
        optimal policy. They're related, but slightly different. Optimal policy
        is the <em>best</em> strategy, while <em>optimal value</em> is the value
        you get at each position after following that strategy.
        <br />
        <br />
        For example, imagine a very difficult position where one brilliant move
        wins but every other move loses. Under Fischer&apos;s optimal policy, v
        <sub>*</sub>(s) for this position is high because his strategy (the
        optimal policy) would find the winning move. But under a beginner&apos;s
        policy, v<sub>beginner</sub>(s) for the same position would be low
        because their strategy (a suboptimal policy) wouldn't find that move,
        they&apos;d play one of the losing alternatives instead.
        <br />
        <br />
        The <strong>optimal action-value function</strong> q<sub>*</sub>
        (s,a) is the maximum action-value function over all policies :
        <br />
        <br />
        <span className="block font-mono text-center">
          q<sub>*</sub>(s, a) = max<sub>œÄ</sub> q<sub>œÄ</sub>(s, a)
        </span>
        <br />
        <br />
        <strong>Chess analogy:</strong> While v<sub>*</sub>(s) tells you "how
        good is this position with perfect play", q<sub>*</sub>(s,a) tells you
        "how good is this specific move, assuming perfect play afterwards". The
        key difference is that v<sub>*</sub>(s) assumes you make the best
        possible move immediately starting from a given position, while q
        <sub>*</sub> (s,a) lets you evaluate ANY move (even bad ones) from a
        given position and see what happens if you play perfectly after that
        move. For example, from a given position, moving your knight to c3 might
        be terrible, moving your bishop to g5 might be brilliant. v<sub>*</sub>
        (s) only tells you the value when you pick the best move (bishop to g5),
        but q<sub>*</sub>(s,a) gives you separate values for BOTH moves: q
        <sub>*</sub>(s, <em>knight to c3</em>) might be low, while q<sub>*</sub>
        (s, <em>bishop to g5</em>) might be high. This lets you compare
        different moves from the same position.
        <br />
        <br />
        <strong>Why this matters:</strong> The optimal value function specifies
        the best possible performance in the MDP. Once we know q<sub>*</sub>,
        we&apos;ve essentially "solved" the MDP, i.e. we know exactly how good
        each state-action pair can possibly be, and therefore we know exactly
        which action to take in any state in order to maximize our total reward
        (we know the optimal policy is just choosing the action with the highest
        q<sub>*</sub>
        every time). The challenge is computing these optimal values, which
        we&apos;ll tackle with the Bellman Optimality Equation.
      </p>

      <p id="bellman-optimality-equation" className="mb-4">
        <strong className="block text-center mb-4">
          Bellman Optimality Equation for MDPs
        </strong>
        We now know that if we somehow already had q<sub>*</sub>, finding the
        optimal policy would be almost unfairly easy: in each state, just pick
        the action with the highest q<sub>*</sub> value. Done.
        <br />
        <br />
        So the real question is: how do we actually find q<sub>*</sub> in the
        first place?
        <br />
        <br />
        This is where the Bellman Optimality Equation enters the story.
        <br />
        <br />
        Recall what the Bellman <em>Expectation</em> Equation does. It&apos;s a
        tool for <em>evaluating</em> a specific policy œÄ. If you hand it a
        policy, it computes the value function for that policy by taking a
        weighted average over actions, where each action is weighted by how
        likely the policy is to choose it, œÄ(a|s). In other words, it answers:
        <br />
        <br />
        <em>
          &quot;If I promise to behave according to this policy, how good will
          my life be in each state?&quot;
        </em>
        <br />
        <br />
        But now we&apos;re after something more ambitious. We don&apos;t want to
        evaluate a policy, we want to <em>find</em> the best one.
        <br />
        <br />
        The Bellman Optimality Equation does that with one small but profound
        change: instead of averaging over actions according to some policy, it
        takes the <strong>maximum</strong> over all possible actions.
        Conceptually, it&apos;s saying:
        <br />
        <br />
        <em>
          &quot;In each state, assume I always make the best possible next
          move.&quot;
        </em>
        <br />
        <br />
        <strong>Recursive Structure For Bellman Optimality Equation</strong>
        <br />
        <br />
        Just like the Bellman Expectation Equation, the Bellman Optimality
        Equation is recursive: q<sub>*</sub>(s, a) is defined in terms of q
        <sub>*</sub>(s', a'). The structure is identical: immediate reward plus
        discounted future value. The only difference is that instead of taking a
        weighted average over actions (according to policy œÄ), we take the
        MAXIMUM over actions. which transforms the equation about evaluating a
        given policy into an equation that characterizes the best possible
        policy.
        <br />
        <br />
        <strong>For the optimal state-value function:</strong>
        <br />
        <br />
        <span className="block font-mono text-center mb-4">
          v<sub>*</sub>(s) = max<sub>a</sub> E[R<sub>t+1</sub> + Œ≥v
          <sub>*</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s, A<sub>t</sub> = a]
        </span>
        This says: the optimal value of being in state s equals the expected
        immediate reward plus the discounted optimal value of wherever you end
        up next, when taking the BEST action. The key difference from the
        expectation equation is when now have the max
        <sub>a</sub>, which means that instead of averaging over a policy&apos;s
        action probabilities (giving certain actions some probability of
        occurring even though they aren't the best), we're now picking the
        action that maximizes the right-hand side of the equation (any actions
        that are not considered the best will get a probability of 0 now).
        <br />
        <br />
        <strong>Chess analogy:</strong> The optimal value v<sub>*</sub>(s) tells
        you how good your current position is assuming you play perfectly from
        here onwards. To compute it, you look at ALL possible moves, evaluate
        the value of each move (immediate reward + discounted optimal value of
        resulting position), and take the maximum. That maximum IS v<sub>*</sub>
        (s), it&apos;s a number representing "how good is this position with
        perfect play".
        <br />
        <br />
        The optimal POLICY œÄ<sub>*</sub>(s) is the actual move that achieves
        this maximum, it&apos;s the decision rule that says "in this position,
        make THIS move." A suboptimal policy might tell you to sacrifice your
        queen for no reason, giving you a low value. But œÄ<sub>*</sub> always
        picks the move that maximizes v<sub>*</sub>.
        <br />
        <br />
        <strong>For the optimal action-value function:</strong>
        <br />
        <br />
        <span className="block font-mono text-center mb-4">
          q<sub>*</sub>(s, a) = E[R<sub>t+1</sub> + Œ≥ max<sub>a'</sub> q
          <sub>*</sub>(S<sub>t+1</sub>, a') | S<sub>t</sub> = s, A<sub>t</sub> =
          a]
        </span>
        This says: the optimal value of taking action a in state s equals the
        expected immediate reward from that action plus the discounted value of
        taking the BEST action in the next state. Notice that the max
        <sub>a'</sub> appears inside the expectation. Conceptually, this means
        that for each possible next state S<sub>t+1</sub> the environment might
        send us to, we assume we will choose the best possible next action a',
        and then we average over all those possible next states.
        <br />
        <br />
        <strong>Chess analogy:</strong> The optimal value of moving your knight
        to c3 equals: the immediate reward from that move (capturing material,
        improving position) plus the discounted value of making the BEST
        possible move in the resulting position. You commit to knight to c3 now,
        but after your opponent responds, you'll choose optimally from whatever
        position results.
        <br />
        <br />
        <strong>Important note:</strong> These are called "optimality" equations
        because we're maximizing over actions rather than following a specific
        policy. They tell us "what&apos;s the best possible value achievable
        from this state?" The policy that achieves these optimal values is
        called the optimal policy œÄ<sub>*</sub>.
      </p>

      <p className="mb-4">
        <strong className="block text-center mb-4">
          The Bellman Optimality Equations: A Visual Breakdown
        </strong>
        Now let&apos;s see how the optimal value functions relate to each other
        through what&apos;s called the Bellman Optimality Equations. These
        equations give us a recursive way to compute optimal values by breaking
        them down into immediate rewards and future optimal values.
        <br />
        <br />
        <strong>
          Equation 1: Optimal State-Value in terms of Optimal Action-Values
        </strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-optimal-v-star.png"
          alt="Bellman Optimality for v*"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <span className="block font-mono text-center mb-4">
          v<sub>*</sub>(s) = max<sub>a</sub> q<sub>*</sub>(s, a)
        </span>
        This diagram shows the one-step lookahead from state s. The hollow
        circle at the top represents state s, and the filled circles below
        represent the different actions you could take from s. The equation
        says: "The optimal value of state s is simply the value of the best
        action you can take from s." In other words, if you're playing
        optimally, you'll always pick the action with the highest q*-value, so
        v*(s) equals that maximum.
        <br />
        <br />
        <strong>Chess analogy:</strong> If you're in a chess position and want
        to know v*(s), the value of that position with perfect play, you look at
        all your possible moves (the filled circles), evaluate each one assuming
        perfect play afterwards (each q*(s,a)), and pick the best. That best
        move&apos;s value IS the position&apos;s value.
        <br />
        <br />
        <strong>
          Equation 2: Optimal Action-Value in terms of Optimal State-Values
        </strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-optimal-q-star.png"
          alt="Bellman Optimality for q*"
          width={500}
          height={300}
          className="mx-auto my-4"
        />
        <span className="block font-mono text-center mb-4">
          q<sub>*</sub>(s, a) = R<sub>s</sub>
          <sup>a</sup> + Œ≥ Œ£<sub>s'‚ààS</sub> P<sub>ss'</sub>
          <sup>a</sup> v<sub>*</sub>(s')
        </span>
        This diagram shows what happens after you take action a from state s.
        The filled circle represents the state-action pair (s,a), and taking
        that action gets you a reward r and transitions you to a next state s'
        (the hollow circle). The equation says: "The optimal value of taking
        action a in state s equals the immediate reward you get, plus the
        discounted expected optimal value of wherever you end up."
      </p>

      <p className="mb-4">
        <strong>Chess analogy:</strong> To evaluate q*(s, move knight to c3),
        you look at: (1) whether this move immediately captures material or
        improves position (the reward r), plus (2) the optimal value of the
        board position that results after the move, accounting for your
        opponent&apos;s possible responses (the Œ£ P<sub>ss'</sub>
        <sup>a</sup> v*(s') term). Even if the move itself doesn't win material,
        it might lead to a position with very high v*(s'), making it the best
        move overall.
      </p>

      <p className="mb-4">
        <strong className="block text-center mb-4">
          The Complete Bellman Optimality Equations: Two Step Lookahead
        </strong>
        Now let&apos;s see what happens when we combine both optimal value
        functions into complete recursive equations. Just like with the Bellman
        Expectation Equations, by substituting one equation into the other, we
        can express each optimal value function entirely in terms of itself!
        <br />
        <br />
        <strong>Optimal State-Value Full Lookahead:</strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-optimal-v-q-together.png"
          alt="Complete Bellman Optimality for v*"
          width={800}
          height={600}
          className="mx-auto mb-4"
        />
        <span className="block font-mono text-center mb-4">
          v<sub>*</sub>(s) = max<sub>a</sub> (R<sub>s</sub>
          <sup>a</sup> + Œ≥ Œ£<sub>s'‚ààS</sub> P<sub>ss'</sub>
          <sup>a</sup> v<sub>*</sub>(s'))
        </span>
        Starting from state s (top hollow circle), we consider all possible
        actions we could take (middle dark circles), and for each action, we
        look at all possible next states s' (bottom hollow circles). But instead
        of averaging over a policy&apos;s action probabilities like in the
        expectation equation, we MAX over actions. This gives us:
        <br />
        <br />
        <strong>Chess analogy:</strong> To find the optimal value of your
        position, you: (1) look at every possible move you could make, (2) for
        each move, evaluate (immediate reward + discounted optimal value of all
        resulting positions weighted by likelihood), and (3) pick whichever move
        gives the highest total. This is a two-step lookahead: position ‚Üí your
        possible moves ‚Üí opponent&apos;s responses (which determine resulting
        positions).
        <br />
        <br />
        <strong>Optimal Action-Value Full Lookahead:</strong>
        <br />
        <br />
        <Image
          src="/images/reinforcement-learning/mdp/mdp-bellman-optimal-q-v-together.png"
          alt="Complete Bellman Optimality for q*"
          width={800}
          height={600}
          className="mx-auto mb-4"
        />
        <span className="block font-mono text-center mb-4">
          q<sub>*</sub>(s, a) = R<sub>s</sub>
          <sup>a</sup> + Œ≥ Œ£<sub>s'‚ààS</sub> P<sub>ss'</sub>
          <sup>a</sup> max<sub>a'</sub> q<sub>*</sub>(s', a')
        </span>
        Starting from state-action pair (s,a) where we've committed to action a
        (top dark circle), the environment transitions us to possible next
        states s' (middle hollow circles), and then from each s', we consider
        all possible next actions a' (bottom dark circles) and MAX over them.
        This gives us:
        <br />
        <br />
        <strong>Chess analogy:</strong> You've decided to move your knight to
        c3. Your opponent responds, creating various positions. From each
        resulting position, you evaluate the best move you could make. The value
        of "knight to c3" is: (immediate reward) + (average over opponent&apos;s
        responses of the best move you can make in each resulting position).
        This is also a two-step lookahead: your move ‚Üí opponent&apos;s response
        ‚Üí your optimal next move.
        <br />
        <br />
        <strong>Key difference from Expectation Equations:</strong> Notice the
        max operator instead of the Œ£ œÄ(a|s) weighting. In the expectation
        equations, we averaged over action probabilities according to a specific
        policy œÄ. In the optimality equations, we simply choose the best action.
        This is what makes these equations tell us about the BEST POSSIBLE
        performance rather than the performance of a specific strategy.
      </p>

      <p className="mb-4">
        At this point, if you&apos;re feeling clever, you might think:
        &quot;Wait a second. Haven&apos;t we seen something like this
        before?&quot;
        <br />
        <br />
        With the Bellman <em>Expectation</em> Equation, life was good. Once we
        fixed a policy œÄ, we could compress the whole MDP into a simple Markov
        Reward Process. The actions disappeared (because we averaged over them
        using œÄ(a|s)), and we were left with a clean, state-only equation:
        <br />
        <br />
        <span className="block font-mono text-center mb-4">
          Iv<sub>œÄ</sub> = r<sub>œÄ</sub> + Œ≥ P<sub>œÄ</sub> v<sub>œÄ</sub>
        </span>
        Which we could rearrange into:
        <br />
        <br />
        <span className="block font-mono text-center mb-4">
          (I ‚àí Œ≥P<sub>œÄ</sub>) v<sub>œÄ</sub> = r<sub>œÄ</sub>
        </span>
        And then solve in one glorious line of linear algebra:
        <br />
        <br />
        <span className="block font-mono text-center mb-4">
          v<sub>œÄ</sub> = (I ‚àí Œ≥P<sub>œÄ</sub>)<sup>‚àí1</sup> r<sub>œÄ</sub>
        </span>
        <br />
        <br />
        So naturally, your brain now tries to pull the exact same stunt here.
        <br />
        <br />
        We have nice, tidy recursive equations for v<sub>*</sub> and q
        <sub>*</sub>. Surely we can just stack all the values into a giant
        vector, write down one massive system of equations, invert a matrix, and
        go home early. Right?
        <br />
        <br />
        WRONG! the moment we replaced &quot;average over actions using a
        policy&quot; with &quot;take the max over actions,&quot; the whole
        equation stopped being linear because the max operator itself isn't
        linear. Now we can&apos;t do our linear algebra tricks to solve the
        system.
      </p>
      <p className="mb-4">
        This brings us to a slightly uncomfortable conclusion.
        <br />
        <br />
        The Bellman Optimality Equation is <em>non-linear</em>. Because of the
        max operator, there is <em>no general closed-form solution</em>. There
        is no magical matrix inverse we can write down. There is no single
        algebraic trick that takes us straight to v<sub>*</sub> or q<sub>*</sub>{" "}
        in one shot.
        <br />
        <br />
        So if we can&apos;t solve these equations directly‚Ä¶ what do we do
        instead?
        <br />
        <br />
        Rather than solving the Bellman Optimality Equation outright, all
        practical reinforcement-learning algorithms take the same basic
        approach: they start with a guess for the value function and then
        repeatedly update that guess until things settle down and stop changing.
        <br />
        <br />
        Over the years, people have come up with several different ways of doing
        exactly this. The four most important ones, and the ones you&apos;ll see
        over and over again in reinforcement learning, are:
        <br />
        <br />
        <strong>Policy Iteration</strong>
        <br />
        <br />
        <strong>Value Iteration</strong>
        <br />
        <br />
        <strong>SARSA</strong>
        <br />
        <br />
        <strong>Q-learning</strong>
        <br />
        <br />
        In the next sections, we&apos;ll go through each of these methods slowly
        and carefully, and you&apos;ll see how they all fall out naturally from
        the same Bellman equations you just met.
      </p>
    </section>
  );
}
